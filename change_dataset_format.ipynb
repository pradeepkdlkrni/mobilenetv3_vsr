{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3af5dde-2e07-47e1-b52f-4373cb82128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import cv2\n",
    "# Define paths\n",
    "SOURCE_DIR = \"F:\\\\vimeo_septuplet_full\\\\sequences\"  # Change this if needed\n",
    "TARGET_DIR = \"F:\\\\vimeo_septuplet_full\\\\Arranged_50\"  # Change this if you want a different location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c80c9f2c-0349-4671-aceb-db5141c6c7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade numpy opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aeed632e-6861-40cc-9fd5-4ed7ed3de5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python-headless in c:\\users\\prade\\.conda\\envs\\mobilenetv3_env\\lib\\site-packages (4.11.0.86)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\prade\\appdata\\roaming\\python\\python39\\site-packages (from opencv-python-headless) (2.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade opencv-python-headless\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a77c5aca-8530-4e61-8f9a-1c59d67dd045",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(os.path.join(TARGET_DIR, \"train/lr\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(TARGET_DIR, \"train/hr\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(TARGET_DIR, \"val/lr\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(TARGET_DIR, \"val/hr\"), exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96db91f-9273-4a5d-b4a1-8dedca9fad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Collect all sequence paths\n",
    "all_sequences = []\n",
    "for seq_folder in sorted(os.listdir(SOURCE_DIR)):  # e.g., 00001, 00002, ...\n",
    "    seq_path = os.path.join(SOURCE_DIR, seq_folder)\n",
    "    if os.path.isdir(seq_path):\n",
    "        for sub_folder in sorted(os.listdir(seq_path)):  # e.g., 0001, 0002, ...\n",
    "            full_path = os.path.join(seq_path, sub_folder)\n",
    "            if os.path.isdir(full_path):\n",
    "                frame_files = [f\"im{i}.png\" for i in range(1, 8)]\n",
    "                if all(os.path.exists(os.path.join(full_path, f)) for f in frame_files):\n",
    "                    all_sequences.append(full_path)\n",
    "\n",
    "# Shuffle and split dataset (90% train, 10% val)\n",
    "random.shuffle(all_sequences)\n",
    "split_index = int(0.9 * len(all_sequences))\n",
    "train_sequences = all_sequences[:split_index]\n",
    "val_sequences = all_sequences[split_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a576d62b-bc65-47f3-8ff8-d3eec5111530",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to process dataset\n",
    "def process_sequences(sequences, split_type):\n",
    "    \"\"\"Copies and renames frames to structured format.\"\"\"\n",
    "    lr_folder = os.path.join(TARGET_DIR, f\"{split_type}/lr\")\n",
    "    hr_folder = os.path.join(TARGET_DIR, f\"{split_type}/hr\")\n",
    "\n",
    "    for seq_path in sequences:\n",
    "        seq_name = \"_\".join(seq_path.split(os.sep)[-2:])  # Format: 00001_0001\n",
    "        os.makedirs(os.path.join(lr_folder, seq_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(hr_folder, seq_name), exist_ok=True)\n",
    "\n",
    "        for i in range(1, 8):  # Frames im1.png to im7.png\n",
    "            img_path = os.path.join(seq_path, f\"im{i}.png\")\n",
    "\n",
    "            # Load HR image\n",
    "            hr_img = cv2.imread(img_path)\n",
    "            h, w, _ = hr_img.shape\n",
    "\n",
    "            # Generate LR image (downscale + upscale for alignment)\n",
    "            lr_img = cv2.resize(hr_img, (w // 4, h // 4), interpolation=cv2.INTER_CUBIC)\n",
    "            lr_img = cv2.resize(lr_img, (w, h), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "            # Save images\n",
    "            hr_filename = f\"frame_{i:05d}.png\"\n",
    "            lr_filename = f\"frame_{i:05d}.png\"\n",
    "\n",
    "            cv2.imwrite(os.path.join(hr_folder, seq_name, hr_filename), hr_img)\n",
    "            cv2.imwrite(os.path.join(lr_folder, seq_name, lr_filename), lr_img)\n",
    "\n",
    "# Process train and val datasets\n",
    "print(\"Processing training set...\")\n",
    "process_sequences(train_sequences, \"train\")\n",
    "\n",
    "print(\"Processing validation set...\")\n",
    "process_sequences(val_sequences, \"val\")\n",
    "\n",
    "print(\"Dataset preprocessing complete! ðŸŽ‰\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f57f80c-8325-4e38-9c1f-7a2a8efb4e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "# Define paths\n",
    "SOURCE_DIR = \"F:\\\\vimeo_septuplet_full\\\\sequences\"  # Original dataset\n",
    "TARGET_DIR = \"F:\\\\vimeo_septuplet_full\\\\Vime90_arranged\"   # New structured dataset\n",
    "\n",
    "# Number of folders to process at a time (Set to 10 for testing, later increase)\n",
    "num_folders_to_process = 10\n",
    "\n",
    "# Ensure directories exist\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    os.makedirs(os.path.join(TARGET_DIR, f\"{split}/lr\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(TARGET_DIR, f\"{split}/hr\"), exist_ok=True)\n",
    "\n",
    "# Collect only a subset of folders for processing\n",
    "all_folders = sorted(os.listdir(SOURCE_DIR))[:num_folders_to_process]  # Process only first N folders\n",
    "\n",
    "# Collect all valid sequences\n",
    "all_sequences = []\n",
    "for seq_folder in all_folders:  # Only process limited folders\n",
    "    seq_path = os.path.join(SOURCE_DIR, seq_folder)\n",
    "    if os.path.isdir(seq_path):\n",
    "        for sub_folder in sorted(os.listdir(seq_path)):\n",
    "            full_path = os.path.join(seq_path, sub_folder)\n",
    "            if os.path.isdir(full_path):\n",
    "                frame_files = [f\"im{i}.png\" for i in range(1, 8)]\n",
    "                if all(os.path.exists(os.path.join(full_path, f)) for f in frame_files):\n",
    "                    all_sequences.append(full_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4c5b6fd-163c-4542-bd5d-32ecd52e4bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Shuffle and split dataset (80% train, 10% val, 10% test)\n",
    "random.shuffle(all_sequences)\n",
    "train_end = int(0.8 * len(all_sequences))\n",
    "val_end = int(0.9 * len(all_sequences))\n",
    "\n",
    "train_sequences = all_sequences[:train_end]\n",
    "val_sequences = all_sequences[train_end:val_end]\n",
    "test_sequences = all_sequences[val_end:]\n",
    "\n",
    "# Function to process sequences\n",
    "def process_sequences(sequences, split_type):\n",
    "    \"\"\"Copies and renames frames to structured format\"\"\"\n",
    "    lr_folder = os.path.join(TARGET_DIR, f\"{split_type}/lr\")\n",
    "    hr_folder = os.path.join(TARGET_DIR, f\"{split_type}/hr\")\n",
    "\n",
    "    for seq_path in sequences:\n",
    "        seq_name = \"_\".join(seq_path.split(os.sep)[-2:])  # e.g., 00001_0001\n",
    "        os.makedirs(os.path.join(lr_folder, seq_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(hr_folder, seq_name), exist_ok=True)\n",
    "\n",
    "        for i in range(1, 8):  # Frames im1.png to im7.png\n",
    "            img_path = os.path.join(seq_path, f\"im{i}.png\")\n",
    "\n",
    "            # Load HR image\n",
    "            hr_img = cv2.imread(img_path)\n",
    "            if hr_img is None:\n",
    "                print(f\"Skipping corrupted/missing image: {img_path}\")\n",
    "                continue\n",
    "\n",
    "            h, w, _ = hr_img.shape\n",
    "\n",
    "            # Generate LR image (downscale + upscale for alignment)\n",
    "            lr_img = cv2.resize(hr_img, (w // 4, h // 4), interpolation=cv2.INTER_CUBIC)\n",
    "            lr_img = cv2.resize(lr_img, (w, h), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "            # Save images\n",
    "            hr_filename = f\"frame_{i:05d}.png\"\n",
    "            lr_filename = f\"frame_{i:05d}.png\"\n",
    "\n",
    "            cv2.imwrite(os.path.join(hr_folder, seq_name, hr_filename), hr_img)\n",
    "            cv2.imwrite(os.path.join(lr_folder, seq_name, lr_filename), lr_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bab03ec-cee4-4334-a7e5-fa72dddf80d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training set...\n",
      "Processing validation set...\n",
      "Processing test set...\n",
      "Dataset preprocessing complete! ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Process datasets\n",
    "print(\"Processing training set...\")\n",
    "process_sequences(train_sequences, \"train\")\n",
    "\n",
    "print(\"Processing validation set...\")\n",
    "process_sequences(val_sequences, \"val\")\n",
    "\n",
    "print(\"Processing test set...\")\n",
    "process_sequences(test_sequences, \"test\")\n",
    "\n",
    "print(\"Dataset preprocessing complete! ðŸŽ‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e87755c6-ebe6-4edc-b108-b43327bf88f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training set...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Process datasets\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing training set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 77\u001b[0m \u001b[43mprocess_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing validation set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     80\u001b[0m process_sequences(val_sequences, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 57\u001b[0m, in \u001b[0;36mprocess_sequences\u001b[1;34m(sequences, split_type)\u001b[0m\n\u001b[0;32m     54\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(seq_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mim\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Load HR image (keep original)\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m hr_img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hr_img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping corrupted/missing image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "# Define paths\n",
    "SOURCE_DIR = \"F:\\\\vimeo_septuplet_full\\\\sequences\"  # Original dataset\n",
    "TARGET_DIR = \"F:\\\\vimeo_septuplet_full\\\\Arranged\"   # New structured dataset\n",
    "\n",
    "# Number of folders to process at a time (Set small first, then increase)\n",
    "num_folders_to_process = 10\n",
    "\n",
    "# Ensure directories exist\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    os.makedirs(os.path.join(TARGET_DIR, f\"{split}/lr\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(TARGET_DIR, f\"{split}/hr\"), exist_ok=True)\n",
    "\n",
    "# Collect only a subset of folders for processing\n",
    "all_folders = sorted(os.listdir(SOURCE_DIR))[:num_folders_to_process]  # Process only first N folders\n",
    "\n",
    "# Collect all valid sequences\n",
    "all_sequences = []\n",
    "for seq_folder in all_folders:\n",
    "    seq_path = os.path.join(SOURCE_DIR, seq_folder)\n",
    "    if os.path.isdir(seq_path):\n",
    "        for sub_folder in sorted(os.listdir(seq_path)):\n",
    "            full_path = os.path.join(seq_path, sub_folder)\n",
    "            if os.path.isdir(full_path):\n",
    "                frame_files = [f\"im{i}.png\" for i in range(1, 8)]\n",
    "                if all(os.path.exists(os.path.join(full_path, f)) for f in frame_files):\n",
    "                    all_sequences.append(full_path)\n",
    "\n",
    "# Shuffle and split dataset (80% train, 10% val, 10% test)\n",
    "random.shuffle(all_sequences)\n",
    "train_end = int(0.8 * len(all_sequences))\n",
    "val_end = int(0.9 * len(all_sequences))\n",
    "\n",
    "train_sequences = all_sequences[:train_end]\n",
    "val_sequences = all_sequences[train_end:val_end]\n",
    "test_sequences = all_sequences[val_end:]\n",
    "\n",
    "# Function to process sequences\n",
    "def process_sequences(sequences, split_type):\n",
    "    \"\"\"Copies HR and generates LR from HR\"\"\"\n",
    "    lr_folder = os.path.join(TARGET_DIR, f\"{split_type}/lr\")\n",
    "    hr_folder = os.path.join(TARGET_DIR, f\"{split_type}/hr\")\n",
    "\n",
    "    for seq_path in sequences:\n",
    "        seq_name = \"_\".join(seq_path.split(os.sep)[-2:])  # e.g., 00001_0001\n",
    "        os.makedirs(os.path.join(lr_folder, seq_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(hr_folder, seq_name), exist_ok=True)\n",
    "\n",
    "        for i in range(1, 8):  # Frames im1.png to im7.png\n",
    "            img_path = os.path.join(seq_path, f\"im{i}.png\")\n",
    "\n",
    "            # Load HR image (keep original)\n",
    "            hr_img = cv2.imread(img_path)\n",
    "            if hr_img is None:\n",
    "                print(f\"Skipping corrupted/missing image: {img_path}\")\n",
    "                continue\n",
    "\n",
    "            h, w, _ = hr_img.shape\n",
    "\n",
    "            # Generate LR image (downscale only)\n",
    "            lr_img = cv2.resize(hr_img, (w // 4, h // 4), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "            # Save HR image (original quality)\n",
    "            hr_filename = f\"frame_{i:05d}.png\"\n",
    "            cv2.imwrite(os.path.join(hr_folder, seq_name, hr_filename), hr_img)\n",
    "\n",
    "            # Save LR image (downscaled only)\n",
    "            lr_filename = f\"frame_{i:05d}.png\"\n",
    "            cv2.imwrite(os.path.join(lr_folder, seq_name, lr_filename), lr_img)\n",
    "\n",
    "# Process datasets\n",
    "print(\"Processing training set...\")\n",
    "process_sequences(train_sequences, \"train\")\n",
    "\n",
    "print(\"Processing validation set...\")\n",
    "process_sequences(val_sequences, \"val\")\n",
    "\n",
    "print(\"Processing test set...\")\n",
    "process_sequences(test_sequences, \"test\")\n",
    "\n",
    "print(\"Dataset preprocessing complete! ðŸŽ‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbf1f5a4-4014-4b36-8147-19a0dffc0c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training set...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Process datasets\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing training set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m \u001b[43mprocess_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_sequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing validation set...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m process_sequences(val_sequences, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 48\u001b[0m, in \u001b[0;36mprocess_sequences\u001b[1;34m(sequences, split_type)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq_path \u001b[38;5;129;01min\u001b[39;00m sequences:\n\u001b[0;32m     47\u001b[0m     seq_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(seq_path\u001b[38;5;241m.\u001b[39msplit(os\u001b[38;5;241m.\u001b[39msep)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])  \u001b[38;5;66;03m# e.g., 00001_0001\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m8\u001b[39m):  \u001b[38;5;66;03m# Frames im1.png to im7.png\u001b[39;00m\n\u001b[0;32m     51\u001b[0m         img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(seq_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mim\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\mobilenetv3_env\\lib\\os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "# Define paths\n",
    "SOURCE_DIR = \"F:\\\\vimeo_septuplet_full\\\\sequences\"  # Original dataset\n",
    "TARGET_DIR = \"F:\\\\vimeo_septuplet_full\\\\vime90k_10folders\"   # Output dataset (only LR)\n",
    "\n",
    "# Number of folders to process at a time (set to small first, then increase)\n",
    "num_folders_to_process = 10  # Change this to process more later\n",
    "\n",
    "# Ensure output directories exist\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    os.makedirs(os.path.join(TARGET_DIR, f\"{split}/lr\"), exist_ok=True)\n",
    "\n",
    "# Collect only a subset of folders for processing\n",
    "all_folders = sorted(os.listdir(SOURCE_DIR))[:num_folders_to_process]  # Process only first N folders\n",
    "\n",
    "# Collect all valid sequences\n",
    "all_sequences = []\n",
    "for seq_folder in all_folders:\n",
    "    seq_path = os.path.join(SOURCE_DIR, seq_folder)\n",
    "    if os.path.isdir(seq_path):\n",
    "        for sub_folder in sorted(os.listdir(seq_path)):\n",
    "            full_path = os.path.join(seq_path, sub_folder)\n",
    "            if os.path.isdir(full_path):\n",
    "                frame_files = [f\"im{i}.png\" for i in range(1, 8)]\n",
    "                if all(os.path.exists(os.path.join(full_path, f)) for f in frame_files):\n",
    "                    all_sequences.append(full_path)\n",
    "\n",
    "# Shuffle and split dataset (80% train, 10% val, 10% test)\n",
    "random.shuffle(all_sequences)\n",
    "train_end = int(0.8 * len(all_sequences))\n",
    "val_end = int(0.9 * len(all_sequences))\n",
    "\n",
    "train_sequences = all_sequences[:train_end]\n",
    "val_sequences = all_sequences[train_end:val_end]\n",
    "test_sequences = all_sequences[val_end:]\n",
    "\n",
    "# Function to process sequences\n",
    "def process_sequences(sequences, split_type):\n",
    "    \"\"\"Copies LR images only, no HR.\"\"\"\n",
    "    lr_folder = os.path.join(TARGET_DIR, f\"{split_type}/lr\")\n",
    "\n",
    "    for seq_path in sequences:\n",
    "        seq_name = \"_\".join(seq_path.split(os.sep)[-2:])  # e.g., 00001_0001\n",
    "        os.makedirs(os.path.join(lr_folder, seq_name), exist_ok=True)\n",
    "\n",
    "        for i in range(1, 8):  # Frames im1.png to im7.png\n",
    "            img_path = os.path.join(seq_path, f\"im{i}.png\")\n",
    "\n",
    "            # Load image\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                print(f\"Skipping corrupted/missing image: {img_path}\")\n",
    "                continue\n",
    "\n",
    "            # Save LR image directly (NO HR)\n",
    "            lr_filename = f\"frame_{i:05d}.png\"\n",
    "            cv2.imwrite(os.path.join(lr_folder, seq_name, lr_filename), img)\n",
    "\n",
    "# Process datasets\n",
    "print(\"Processing training set...\")\n",
    "process_sequences(train_sequences, \"train\")\n",
    "\n",
    "print(\"Processing validation set...\")\n",
    "process_sequences(val_sequences, \"val\")\n",
    "\n",
    "print(\"Processing test set...\")\n",
    "process_sequences(test_sequences, \"test\")\n",
    "\n",
    "print(\"Dataset preprocessing complete! ðŸŽ‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f172b0ee-e9db-4a5d-9022-2ac41ed57b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define paths\n",
    "source_dir = r\"F:\\\\vimeo_septuplet_full\\\\sequences\"\n",
    "output_dir = r\"F:\\vimeo_mobilenetv3_dataset\"\n",
    "\n",
    "# Process only a limited number of folders\n",
    "max_folders_to_process = 10  # Adjust this number as needed\n",
    "\n",
    "# Define split ratios\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.15\n",
    "test_ratio = 0.15\n",
    "\n",
    "# Downsampling factor for creating LR images\n",
    "scale_factor = 4  # Common scale factor for super-resolution (4x)\n",
    "\n",
    "# Create output directories\n",
    "def create_directory_structure():\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        for res in ['lr', 'hr']:\n",
    "            os.makedirs(os.path.join(output_dir, f\"{split}-{res}\"), exist_ok=True)\n",
    "    \n",
    "    print(\"Directory structure created successfully.\")\n",
    "\n",
    "# Function to downsample an image\n",
    "def downsample_image(image_path, scale):\n",
    "    img = cv2.imread(image_path)\n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    # Calculate new dimensions\n",
    "    new_h, new_w = h // scale, w // scale\n",
    "    \n",
    "    # Resize down\n",
    "    lr_img = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    # For training purposes, we sometimes resize back to original size to match dimensions\n",
    "    # Uncomment below if you want LR images to be the same size as HR but lower quality\n",
    "    # lr_img = cv2.resize(lr_img, (w, h), interpolation=cv2.INTER_CUBIC)\n",
    "    \n",
    "    return lr_img\n",
    "\n",
    "# Function to process and split the dataset\n",
    "def split_dataset():\n",
    "    # Get a subset of sequence folders\n",
    "    sequence_folders = []\n",
    "    for folder in os.listdir(source_dir):\n",
    "        folder_path = os.path.join(source_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            sequence_folders.append(folder)\n",
    "    \n",
    "    # Shuffle and limit to max_folders_to_process\n",
    "    random.shuffle(sequence_folders)\n",
    "    sequence_folders = sequence_folders[:max_folders_to_process]\n",
    "    \n",
    "    print(f\"Processing {len(sequence_folders)} folders out of the full dataset\")\n",
    "    \n",
    "    # Calculate split indices\n",
    "    total_folders = len(sequence_folders)\n",
    "    train_count = int(total_folders * train_ratio)\n",
    "    val_count = int(total_folders * val_ratio)\n",
    "    \n",
    "    train_folders = sequence_folders[:train_count]\n",
    "    val_folders = sequence_folders[train_count:train_count+val_count]\n",
    "    test_folders = sequence_folders[train_count+val_count:]\n",
    "    \n",
    "    print(f\"Dataset will be split into: {len(train_folders)} train, {len(val_folders)} val, {len(test_folders)} test folders\")\n",
    "    \n",
    "    # Process each split\n",
    "    process_split(train_folders, 'train')\n",
    "    process_split(val_folders, 'val')\n",
    "    process_split(test_folders, 'test')\n",
    "\n",
    "# Process a specific split (train/val/test)\n",
    "def process_split(folders, split_name):\n",
    "    print(f\"Processing {split_name} split...\")\n",
    "    \n",
    "    # Track sequence count per split\n",
    "    sequence_count = 0\n",
    "    image_count = 0\n",
    "    \n",
    "    # Use tqdm for progress tracking\n",
    "    for folder in tqdm(folders, desc=f\"{split_name.capitalize()} Split\"):\n",
    "        folder_path = os.path.join(source_dir, folder)\n",
    "        \n",
    "        # Get a limited number of subfolders per folder (to further limit processing)\n",
    "        sub_folders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "        max_sub_per_folder = min(5, len(sub_folders))  # Process at most 5 subfolders per folder\n",
    "        sub_folders = sub_folders[:max_sub_per_folder]\n",
    "        \n",
    "        for sub_folder in sub_folders:\n",
    "            sub_folder_path = os.path.join(folder_path, sub_folder)\n",
    "            images = sorted([f for f in os.listdir(sub_folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "            \n",
    "            # For video super resolution, we typically use sequences\n",
    "            if len(images) < 3:  # Skip if not enough images for a sequence\n",
    "                continue\n",
    "            \n",
    "            sequence_count += 1\n",
    "            \n",
    "            # Create unique identifier for this sequence\n",
    "            sequence_id = f\"{folder}_{sub_folder}\"\n",
    "            \n",
    "            # Process each image in the sequence\n",
    "            for img_name in images:\n",
    "                img_path = os.path.join(sub_folder_path, img_name)\n",
    "                \n",
    "                # Copy original image to HR directory\n",
    "                hr_dest_path = os.path.join(output_dir, f\"{split_name}-hr\", f\"{sequence_id}_{img_name}\")\n",
    "                shutil.copy(img_path, hr_dest_path)\n",
    "                \n",
    "                # Create and save downsampled version to LR directory\n",
    "                lr_img = downsample_image(img_path, scale_factor)\n",
    "                lr_dest_path = os.path.join(output_dir, f\"{split_name}-lr\", f\"{sequence_id}_{img_name}\")\n",
    "                cv2.imwrite(lr_dest_path, lr_img)\n",
    "                \n",
    "                image_count += 1\n",
    "    \n",
    "    print(f\"Processed {sequence_count} sequences with {image_count} total images for {split_name} split\")\n",
    "\n",
    "# Calculate dataset statistics\n",
    "def calculate_stats():\n",
    "    print(\"\\nCalculating dataset statistics...\")\n",
    "    \n",
    "    stats = {}\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        hr_dir = os.path.join(output_dir, f\"{split}-hr\")\n",
    "        lr_dir = os.path.join(output_dir, f\"{split}-lr\")\n",
    "        \n",
    "        hr_count = len([f for f in os.listdir(hr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        lr_count = len([f for f in os.listdir(lr_dir) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "        \n",
    "        stats[split] = {'hr': hr_count, 'lr': lr_count}\n",
    "        \n",
    "        print(f\"{split.capitalize()} set: {hr_count} HR images, {lr_count} LR images\")\n",
    "    \n",
    "    # Verify a sample image to confirm downsampling\n",
    "    if stats['train']['hr'] > 0:\n",
    "        sample_hr = os.path.join(output_dir, \"train-hr\", os.listdir(os.path.join(output_dir, \"train-hr\"))[0])\n",
    "        sample_lr = os.path.join(output_dir, \"train-lr\", os.listdir(os.path.join(output_dir, \"train-lr\"))[0])\n",
    "        \n",
    "        hr_img = cv2.imread(sample_hr)\n",
    "        lr_img = cv2.imread(sample_lr)\n",
    "        \n",
    "        print(f\"\\nSample HR image shape: {hr_img.shape}\")\n",
    "        print(f\"Sample LR image shape: {lr_img.shape}\")\n",
    "        print(f\"Resolution reduction: {scale_factor}x\")\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    print(f\"Starting Vimeo-90K SUBSET preparation for MobileNetV3 video super resolution\")\n",
    "    print(f\"Source directory: {source_dir}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print(f\"Processing only {max_folders_to_process} folders to save time\")\n",
    "    print(f\"Downsampling factor: {scale_factor}x\")\n",
    "    \n",
    "    # Create directories\n",
    "    create_directory_structure()\n",
    "    \n",
    "    # Split and process the dataset\n",
    "    split_dataset()\n",
    "    \n",
    "    # Calculate statistics\n",
    "    calculate_stats()\n",
    "    \n",
    "    print(\"\\nSubset dataset preparation complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b3f5a3-9871-40cf-a0c3-8d77861520db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68ebc6-fe10-4839-959d-b312e050b916",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################use below one #####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d3a993-6328-46cf-8f73-adcf19e1f802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing training set...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "# Define paths\n",
    "SOURCE_DIR = \"F:\\\\vimeo_septuplet_full\\\\sequences\"  # Original dataset\n",
    "TARGET_DIR = \"F:\\\\vimeo_septuplet_full\\\\Arranged_full\"   # Output dataset (LR + HR)\n",
    "\n",
    "# Number of folders to process at a time (set to small first, then increase)\n",
    "num_folders_to_process = 95  # Change this to process more later\n",
    "\n",
    "# Ensure output directories exist\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    os.makedirs(os.path.join(TARGET_DIR, f\"{split}/lr\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(TARGET_DIR, f\"{split}/hr\"), exist_ok=True)\n",
    "\n",
    "# Collect only a subset of folders for processing\n",
    "all_folders = sorted(os.listdir(SOURCE_DIR))[:num_folders_to_process]  # Process only first N folders\n",
    "\n",
    "# Collect all valid sequences\n",
    "all_sequences = []\n",
    "for seq_folder in all_folders:\n",
    "    seq_path = os.path.join(SOURCE_DIR, seq_folder)\n",
    "    if os.path.isdir(seq_path):\n",
    "        for sub_folder in sorted(os.listdir(seq_path)):\n",
    "            full_path = os.path.join(seq_path, sub_folder)\n",
    "            if os.path.isdir(full_path):\n",
    "                frame_files = [f\"im{i}.png\" for i in range(1, 8)]\n",
    "                if all(os.path.exists(os.path.join(full_path, f)) for f in frame_files):\n",
    "                    all_sequences.append(full_path)\n",
    "\n",
    "# Shuffle and split dataset (80% train, 10% val, 10% test)\n",
    "random.shuffle(all_sequences)\n",
    "train_end = int(0.8 * len(all_sequences))\n",
    "val_end = int(0.9 * len(all_sequences))\n",
    "\n",
    "train_sequences = all_sequences[:train_end]\n",
    "val_sequences = all_sequences[train_end:val_end]\n",
    "test_sequences = all_sequences[val_end:]\n",
    "\n",
    "# Function to process sequences\n",
    "def process_sequences(sequences, split_type):\n",
    "    \"\"\"Copies HR and generates LR from HR\"\"\"\n",
    "    lr_folder = os.path.join(TARGET_DIR, f\"{split_type}/lr\")\n",
    "    hr_folder = os.path.join(TARGET_DIR, f\"{split_type}/hr\")\n",
    "\n",
    "    for seq_path in sequences:\n",
    "        seq_name = \"_\".join(seq_path.split(os.sep)[-2:])  # e.g., 00001_0001\n",
    "        os.makedirs(os.path.join(lr_folder, seq_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(hr_folder, seq_name), exist_ok=True)\n",
    "\n",
    "        for i in range(1, 8):  # Frames im1.png to im7.png\n",
    "            img_path = os.path.join(seq_path, f\"im{i}.png\")\n",
    "\n",
    "            # Load HR image (keep original)\n",
    "            hr_img = cv2.imread(img_path)\n",
    "            if hr_img is None:\n",
    "                print(f\"Skipping corrupted/missing image: {img_path}\")\n",
    "                continue\n",
    "\n",
    "            h, w, _ = hr_img.shape\n",
    "\n",
    "            # Generate LR image (downscale only)\n",
    "            lr_img = cv2.resize(hr_img, (w // 4, h // 4), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "            # Save HR image (original quality)\n",
    "            hr_filename = f\"frame_{i:05d}.png\"\n",
    "            cv2.imwrite(os.path.join(hr_folder, seq_name, hr_filename), hr_img)\n",
    "\n",
    "            # Save LR image (downscaled version)\n",
    "            lr_filename = f\"frame_{i:05d}.png\"\n",
    "            cv2.imwrite(os.path.join(lr_folder, seq_name, lr_filename), lr_img)\n",
    "\n",
    "# Process datasets\n",
    "print(\"Processing training set...\")\n",
    "process_sequences(train_sequences, \"train\")\n",
    "\n",
    "print(\"Processing validation set...\")\n",
    "process_sequences(val_sequences, \"val\")\n",
    "\n",
    "print(\"Processing test set...\")\n",
    "process_sequences(test_sequences, \"test\")\n",
    "\n",
    "print(\"Dataset preprocessing complete! ðŸŽ‰\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fdf1bb-a67c-4865-9cce-ced92da468e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
